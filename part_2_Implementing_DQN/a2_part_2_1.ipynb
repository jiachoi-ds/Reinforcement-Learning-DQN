{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Q-Network (DQN) Implementation for Grid-World Environment\n",
        "## Assignment 2.1: Implementing DQN from Scratch\n",
        "\n",
        "This notebook implements DQN based on the DeepMind paper:\n",
        "- Mnih et al. (2015) \"Human-level control through deep reinforcement learning\" Nature\n",
        "\n",
        "The implementation applies DQN to solve the drone delivery grid-world environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# PyTorch for deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Gymnasium for RL environment\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Grid-World Environment Definition\n",
        "\n",
        "Using the stochastic environment from Assignment 1 with wind effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Definition of the Grid Environment class.\n",
        "\n",
        "class GridEnvironment_stochastic(gym.Env):\n",
        "    # Attribute of a Gym class that provides info about the render modes\n",
        "    metadata = {'render.modes': [\"human\"]}\n",
        "\n",
        "    # Initialization function\n",
        "    def __init__(self):\n",
        "\n",
        "      self.observation_space = spaces.Discrete(6*6*2)\n",
        "      self.action_space = spaces.Discrete(6) # up, down, left, right, pick-up, drop-off\n",
        "\n",
        "      self.max_timesteps = 20\n",
        "      self.timestep = 0\n",
        "\n",
        "      self.agent_pos = [0,0]\n",
        "      self.start_pos = [0,0]\n",
        "      self.goal_pos =  [4,4]\n",
        "      self.obstacles = {(1,4), (2,3), (3,3)}\n",
        "      self.state = np.zeros((6,6))\n",
        "      self.state[tuple(self.agent_pos)] = 1\n",
        "      self.state[tuple(self.goal_pos)] = 0.5\n",
        "      self.package = False\n",
        "      self.first_pickup = False\n",
        "\n",
        "    # Reset function\n",
        "    def reset(self, **kwargs):\n",
        "      self.timestep = 0\n",
        "      self.agent_pos = [0,0]\n",
        "      self.start_pos = [0,0]\n",
        "      self.package = False\n",
        "      self.first_pickup = False\n",
        "      self.state = np.zeros((6,6))\n",
        "      self.state[tuple(self.agent_pos)] = 1\n",
        "      self.state[tuple(self.goal_pos)] = 0.5\n",
        "      for obs in self.obstacles:\n",
        "        self.state[obs] = 0.3\n",
        "      observation = self.state.flatten()\n",
        "\n",
        "      info = {}\n",
        "\n",
        "      return observation, info\n",
        "\n",
        "    # Step function: Contains the implementation for what happens when an\n",
        "    # agent takes a step in the environment.\n",
        "    def step(self, action):\n",
        "\n",
        "      reward = 0\n",
        "      reward -= 1\n",
        "      terminated = False\n",
        "      truncated = False\n",
        "\n",
        "      success = 0.9\n",
        "\n",
        "      def apply_wind(original, deviation):\n",
        "        if np.random.rand() < success:\n",
        "            return original\n",
        "        else:\n",
        "            # pick one of lateral moves randomly\n",
        "            return deviation[np.random.choice(len(deviation))]\n",
        "\n",
        "\n",
        "      if action == 0:  # down\n",
        "          next_position = apply_wind(\n",
        "              [self.agent_pos[0] + 1, self.agent_pos[1]],\n",
        "              [[self.agent_pos[0], self.agent_pos[1] - 1],  # left dev\n",
        "              [self.agent_pos[0], self.agent_pos[1] + 1]]  # right dev\n",
        "          )\n",
        "          next_position[0] = max(0, min(5, next_position[0]))\n",
        "          next_position[1] = max(0, min(5, next_position[1]))\n",
        "          if tuple(next_position) in self.obstacles:\n",
        "              reward -= 100\n",
        "          else:\n",
        "              self.agent_pos = next_position\n",
        "\n",
        "      if action == 1:  # up\n",
        "          next_position = apply_wind(\n",
        "              [self.agent_pos[0] - 1, self.agent_pos[1]],\n",
        "              [[self.agent_pos[0], self.agent_pos[1] - 1],\n",
        "              [self.agent_pos[0], self.agent_pos[1] + 1]]\n",
        "          )\n",
        "          next_position[0] = max(0, min(5, next_position[0]))\n",
        "          next_position[1] = max(0, min(5, next_position[1]))\n",
        "          if tuple(next_position) in self.obstacles:\n",
        "              reward -= 100\n",
        "          else:\n",
        "              self.agent_pos = next_position\n",
        "\n",
        "      if action == 2:  # right\n",
        "          next_position = apply_wind(\n",
        "              [self.agent_pos[0], self.agent_pos[1] + 1],\n",
        "              [[self.agent_pos[0] + 1, self.agent_pos[1]],  # down dev\n",
        "              [self.agent_pos[0] - 1, self.agent_pos[1]]]  # up dev\n",
        "          )\n",
        "          next_position[0] = max(0, min(5, next_position[0]))\n",
        "          next_position[1] = max(0, min(5, next_position[1]))\n",
        "          if tuple(next_position) in self.obstacles:\n",
        "              reward -= 100\n",
        "          else:\n",
        "              self.agent_pos = next_position\n",
        "\n",
        "      if action == 3:  # left\n",
        "          next_position = apply_wind(\n",
        "              [self.agent_pos[0], self.agent_pos[1] - 1],\n",
        "              [[self.agent_pos[0] + 1, self.agent_pos[1]],\n",
        "              [self.agent_pos[0] - 1, self.agent_pos[1]]]\n",
        "          )\n",
        "          next_position[0] = max(0, min(5, next_position[0]))\n",
        "          next_position[1] = max(0, min(5, next_position[1]))\n",
        "          if tuple(next_position) in self.obstacles:\n",
        "              reward -= 100\n",
        "          else:\n",
        "              self.agent_pos = next_position\n",
        "\n",
        "      if action == 4:\n",
        "        if np.array_equal(self.agent_pos, self.start_pos) and not self.package:\n",
        "          self.package = True\n",
        "          if not self.first_pickup:\n",
        "            reward += 25\n",
        "            self.first_pickup = True\n",
        "\n",
        "      if action == 5:\n",
        "         if np.array_equal(self.agent_pos, self.goal_pos) and self.package:\n",
        "          reward += 100\n",
        "          terminated = True\n",
        "          self.package = False\n",
        "\n",
        "      self.agent_pos = np.clip(self.agent_pos, 0, 5)\n",
        "\n",
        "      self.state = np.zeros((6,6))\n",
        "      self.state[tuple(self.agent_pos)] = 1\n",
        "      self.state[tuple(self.goal_pos)] = 0.5\n",
        "      for obs in self.obstacles:\n",
        "        self.state[obs] = 0.3\n",
        "      observation = self.state.flatten()\n",
        "\n",
        "      self.timestep += 1\n",
        "\n",
        "      if self.timestep >= self.max_timesteps and not terminated:\n",
        "        truncated = True\n",
        "\n",
        "      info = {}\n",
        "\n",
        "      return observation, reward, terminated, truncated, info\n",
        "\n",
        "    # Render function: Visualizes the environment\n",
        "    def render(self):\n",
        "\n",
        "      grid = np.zeros((6, 6))\n",
        "\n",
        "      for r, c in self.obstacles:\n",
        "          grid[r, c] = 0.3\n",
        "\n",
        "      gr, gc = self.goal_pos\n",
        "      grid[gr, gc] = 0.6\n",
        "\n",
        "\n",
        "      sr, sc = self.start_pos\n",
        "      grid[sr, sc] = 0.8\n",
        "\n",
        "\n",
        "      ar, ac = self.agent_pos\n",
        "\n",
        "\n",
        "      plt.figure(figsize=(4, 4))\n",
        "      im = plt.imshow(grid, cmap=\"viridis\", vmin=0, vmax=1)\n",
        "\n",
        "      plt.scatter(ac, ar, s=200, c='red', marker='x',\n",
        "                linewidths=3 if self.package else 1.5)\n",
        "\n",
        "\n",
        "      legend_elements = [\n",
        "          mpatches.Patch(color=im.cmap(0.8), label=\"Warehouse\"),\n",
        "          mpatches.Patch(color=im.cmap(0.6), label=\"Customer\"),\n",
        "          mpatches.Patch(color=im.cmap(0.3), label=\"Obstacle\"),\n",
        "          plt.Line2D([0], [0], marker='x', color='red', label='Drone',\n",
        "                    markersize=10, linewidth=0)\n",
        "      ]\n",
        "\n",
        "      plt.legend(handles=legend_elements,\n",
        "               bbox_to_anchor=(1.05, 1),\n",
        "               loc=\"upper left\",\n",
        "               borderaxespad=0.)\n",
        "\n",
        "\n",
        "      plt.xticks(range(6))\n",
        "      plt.yticks(range(6))\n",
        "\n",
        "\n",
        "      plt.title(f\"Drone delivery\")\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "env_stochastic = GridEnvironment_stochastic()\n",
        "\n",
        "terminated, truncated = False, False\n",
        "obs, info = env_stochastic.reset()\n",
        "env_stochastic.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. MLP-based Deep Q-Network Architecture\n",
        "\n",
        "**Reference: Human-level control through deep reinforcement learning (Mnih et al., 2015)**\n",
        "\n",
        "we use MLP for vector input `[position_x, position_y, package]`.\n",
        "\n",
        "The network follows the principles from the Human-level paper:\n",
        "- Multiple fully-connected layers with ReLU activations\n",
        "- Output layer produces Q-values for each action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_state(env):\n",
        "    \"\"\"\n",
        "    Convert environment state to vector format: [position_x, position_y, package]\n",
        "    \n",
        "    Reference: Similar to vector observation type 2.1 in assignment part 1\n",
        "    \n",
        "    Args:\n",
        "        env: GridEnvironment_stochastic instance\n",
        "    \n",
        "    Returns:\n",
        "        state_vector: numpy array of shape (3,) with normalized values [0, 1]\n",
        "    \"\"\"\n",
        "    # Get agent position and normalize to [0, 1]\n",
        "    agent_x = env.agent_pos[0] / 5.0  # Normalize x-coordinate (grid is 6x6, so 0-5)\n",
        "    agent_y = env.agent_pos[1] / 5.0  # Normalize y-coordinate\n",
        "    package = float(env.package)      # Convert boolean to float (0.0 or 1.0)\n",
        "    \n",
        "    # Create state vector: [x, y, package]\n",
        "    state_vector = np.array([agent_x, agent_y, package], dtype=np.float32)\n",
        "    \n",
        "    return state_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transition tuple for storing experiences\n",
        "# Reference: Methods section in Mnih et al. (2015)\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Experience Replay Memory for storing and sampling transitions.\n",
        "    \n",
        "    Reference: Methods section \"Experience Replay\" in Mnih et al. (2015)\n",
        "    The agent stores the last N experience tuples in the replay memory,\n",
        "    and samples uniformly at random from the pool of stored samples.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, capacity):\n",
        "        \"\"\"\n",
        "        Initialize replay memory with fixed capacity.\n",
        "        \n",
        "        Args:\n",
        "            capacity (int): Maximum number of transitions to store\n",
        "                           Paper uses 1M for Atari, we use smaller for grid-world\n",
        "        \"\"\"\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "    \n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition to memory.\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a random batch of transitions.\n",
        "        \n",
        "        Reference: \"samples are drawn uniformly at random from the pool\"\n",
        "        (Methods section, Mnih et al. 2015)\n",
        "        \"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DQN_MLP(\n",
              "  (fc1): Linear(in_features=3, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc4): Linear(in_features=64, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class DQN_MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network with MLP architecture for vector-based state representation.\n",
        "    \n",
        "    Reference: Mnih et al. (2015) - Human-level control through deep reinforcement learning\n",
        "    \"The final hidden layer is fully-connected and consists of 512 rectifier units.\"\n",
        "    \n",
        "    Adapted from CNN (for images) to MLP (for vector input: position_x, position_y, package).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size=3, hidden_size=512, output_size=6):\n",
        "        \"\"\"\n",
        "        Initialize the MLP-based DQN.\n",
        "        \n",
        "        Args:\n",
        "            input_size (int): Size of input vector (3 for [x, y, package])\n",
        "            hidden_size (int): Number of neurons in hidden layers (512 as in paper)\n",
        "            output_size (int): Number of actions (6: up, down, left, right, pickup, dropoff)\n",
        "        \"\"\"\n",
        "        super(DQN_MLP, self).__init__()\n",
        "        \n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size*2)\n",
        "        self.fc3 = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the MLP network.\n",
        "        \n",
        "        Reference: \"applies a rectifier nonlinearity\" (ReLU activations in Mnih et al. 2015)\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 3) for [position_x, position_y, package]\n",
        "            \n",
        "        Returns:\n",
        "            Q-values for each action, shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "            \n",
        "        # Forward pass with ReLU activations\n",
        "        # Reference: ReLU nonlinearity as in Human-level paper\n",
        "        x = F.relu(self.fc1(x))    # input_size → hidden_size (3 → 512)\n",
        "        x = F.relu(self.fc2(x))    # hidden_size → hidden_size (512 → 512)\n",
        "        x = F.relu(self.fc3(x))    # hidden_size → hidden_size (512 → 512)\n",
        "        x = self.fc4(x)            # hidden_size → output_size (512 → 6), linear output (Q-values)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Initialize MLP-based networks\n",
        "input_size = 3       # 3 features: [position_x, position_y, package]\n",
        "hidden_size = 64  \n",
        "output_size = 6      # 6 actions\n",
        "\n",
        "policy_net_mlp = DQN_MLP(input_size, hidden_size, output_size).to(device)\n",
        "target_net_mlp = DQN_MLP(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# Initialize target network with same weights as policy network\n",
        "# Reference: \"target network parameters are only updated with the main network parameters every C steps\"\n",
        "# (Methods section, Mnih et al. 2015)\n",
        "target_net_mlp.load_state_dict(policy_net_mlp.state_dict())\n",
        "target_net_mlp.eval()  # Target network is only used for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. DQN Agent for MLP-based Network\n",
        "\n",
        "**Reference: Algorithm 1 \"Deep Q-learning with Experience Replay\" in Mnih et al. (2015)**\n",
        "\n",
        "Key modifications from CNN version:\n",
        "- Input: vector `[position_x, position_y, package]` instead of grid image\n",
        "- Uses MLP network instead of CNN\n",
        "- Same core DQN algorithm (experience replay, target network, epsilon-greedy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent_MLP:\n",
        "    \"\"\"\n",
        "    DQN Agent with MLP network for vector-based state representation.\n",
        "    \n",
        "    Reference: Mnih et al. (2015) - Human-level control through deep reinforcement learning\n",
        "    \n",
        "    Key components:\n",
        "    1. Experience Replay (Methods section, Mnih et al. 2015)\n",
        "    2. Target Network (Methods section, Mnih et al. 2015)\n",
        "    3. Epsilon-greedy exploration (Methods section, Mnih et al. 2015)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, action_size,\n",
        "                 hidden_size=512,         # Reference: Network Architecture in Mnih et al. 2015\n",
        "                 learning_rate=0.00025,  # Reference: Methods section, Mnih et al. 2015\n",
        "                 gamma=0.99,              # Reference: Discount factor γ in paper\n",
        "                 epsilon_start=1.0,\n",
        "                 epsilon_end=0.1,         # Reference: ε annealed to 0.1 (Mnih et al. 2015)\n",
        "                 epsilon_decay_steps=1000000,  # Reference: over 1M frames\n",
        "                 memory_size=100000,      # Reference: 1M in paper, scaled down\n",
        "                 batch_size=32,           # Reference: Mini-batch size in paper\n",
        "                 target_update=10000,     # Reference: C=10k updates (Mnih et al. 2015)\n",
        "                 reward_clip=False):       # Reference: Reward clipping to [-1,1]\n",
        "        \"\"\"\n",
        "        Initialize DQN Agent with MLP network.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.action_size = action_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update\n",
        "        self.reward_clip = reward_clip\n",
        "        self.update_counter = 0\n",
        "        self.step_counter = 0\n",
        "        \n",
        "        # Initialize MLP-based networks\n",
        "        self.policy_net = DQN_MLP(input_size, self.hidden_size, action_size).to(device)\n",
        "        self.target_net = DQN_MLP(input_size, self.hidden_size, action_size).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        \n",
        "        # Reference: \"RMSProp algorithm with minibatches of size 32\" (Methods, Mnih et al. 2015)\n",
        "        self.optimizer = optim.RMSprop(self.policy_net.parameters(), lr=learning_rate, alpha=0.95, eps=0.01)\n",
        "        \n",
        "        # Initialize replay memory\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "        \n",
        "        # For tracking\n",
        "        self.loss_history = []\n",
        "        \n",
        "        # Warm-up threshold\n",
        "        self.replay_warmup = 1000\n",
        "        \n",
        "    def select_action(self, state, training=True):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy policy.\n",
        "        \n",
        "        Reference: \"The behavior policy during training was ε-greedy\"\n",
        "        (Methods section, Mnih et al. 2015)\n",
        "        \"\"\"\n",
        "        epsilon = self.epsilon if training else 0.05\n",
        "        \n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                # state is already a vector [x, y, package]\n",
        "                state_tensor = torch.FloatTensor(state).to(device)\n",
        "                q_values = self.policy_net(state_tensor)\n",
        "                return q_values.argmax().item()\n",
        "    \n",
        "    def store_transition(self, state, action, next_state, reward, done):\n",
        "        \"\"\"\n",
        "        Store transition in replay memory.\n",
        "        \n",
        "        Reference: \"store the agent's experiences et = (st,at,rt,st+1)\"\n",
        "        (Methods section, Mnih et al. 2015)\n",
        "        \"\"\"\n",
        "        self.memory.push(state, action, next_state, reward, done)\n",
        "    \n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Perform one training step using experience replay.\n",
        "        \n",
        "        Reference: Methods section in Mnih et al. (2015)\n",
        "        \"\"\"\n",
        "        if len(self.memory) < max(self.batch_size, self.replay_warmup):\n",
        "            return None\n",
        "        \n",
        "        # Sample random mini-batch\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        \n",
        "        # Convert to tensors\n",
        "        state_batch = torch.FloatTensor(np.array(batch.state)).to(device)\n",
        "        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(device)\n",
        "        reward_batch = torch.FloatTensor(batch.reward).to(device)\n",
        "        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(device)\n",
        "        done_batch = torch.FloatTensor(batch.done).to(device)\n",
        "        \n",
        "        # Compute Q(s_t, a)\n",
        "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "        \n",
        "        # Compute target Q-values\n",
        "        # Reference: \"set yj = rj if episode terminates; otherwise yj = rj + γ max Q̂(φj+1,a';θ−)\"\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_net(next_state_batch).max(1)[0]\n",
        "            target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
        "        \n",
        "        # Compute Huber loss\n",
        "        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update target network\n",
        "        self.update_counter += 1\n",
        "        if self.update_counter % self.target_update == 0:\n",
        "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "        self.loss_history.append(loss.item())\n",
        "        return loss.item()\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decay epsilon linearly.\n",
        "        \n",
        "        Reference: \"ε was annealed linearly from 1 to 0.1\"\n",
        "        (Methods section, Mnih et al. 2015)\n",
        "        \"\"\"\n",
        "        self.step_counter += 1\n",
        "        if self.step_counter <= self.epsilon_decay_steps:\n",
        "            self.epsilon = self.epsilon_start - (self.epsilon_start - self.epsilon_end) * (self.step_counter / self.epsilon_decay_steps)\n",
        "        else:\n",
        "            self.epsilon = self.epsilon_end\n",
        "    \n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"Save trained model weights.\"\"\"\n",
        "        torch.save({\n",
        "            'policy_net_state_dict': self.policy_net.state_dict(),\n",
        "            'target_net_state_dict': self.target_net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon\n",
        "        }, filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "    \n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"Load trained model weights.\"\"\"\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "        print(f\"Model loaded from {filepath}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_dqn_mlp(env, agent, n_episodes=1000, max_steps=20):\n",
        "    \"\"\"\n",
        "    Train DQN agent with MLP network using vector state representation.\n",
        "    \n",
        "    Reference: Methods section in Mnih et al. (2015)\n",
        "    Main training loop implementing experience replay and target network updates.\n",
        "    \n",
        "    Args:\n",
        "        env: GridEnvironment_stochastic instance\n",
        "        agent: DQNAgent_MLP instance\n",
        "        n_episodes: Number of training episodes\n",
        "        max_steps: Maximum steps per episode\n",
        "    \n",
        "    Returns:\n",
        "        Training statistics (rewards, lengths, losses, epsilon values)\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    episode_losses = []\n",
        "    epsilon_values = []\n",
        "    \n",
        "    print(\"Starting MLP-based DQN Training...\")\n",
        "    \n",
        "    for episode in range(n_episodes):\n",
        "        # Reset environment\n",
        "        observation, _ = env.reset()\n",
        "        \n",
        "        # Convert observation to vector state [x, y, package]\n",
        "        state = preprocess_state(env)\n",
        "        \n",
        "        total_reward = 0\n",
        "        episode_loss = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action using epsilon-greedy policy\n",
        "            action = agent.select_action(state, training=True)\n",
        "            # Decay epsilon after each episode\n",
        "            agent.decay_epsilon()\n",
        "            \n",
        "            # Execute action in environment\n",
        "            next_observation, reward, done, truncated, _ = env.step(action)\n",
        "            \n",
        "            # Convert next observation to vector state\n",
        "            next_state = preprocess_state(env)\n",
        "            \n",
        "            # Store transition in replay memory\n",
        "            agent.store_transition(state, action, next_state, reward, done or truncated)\n",
        "            \n",
        "            # Train agent\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_loss.append(loss)\n",
        "            \n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done or truncated:\n",
        "                break\n",
        "        \n",
        "\n",
        "        \n",
        "        # Record statistics\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_lengths.append(step + 1)\n",
        "        episode_losses.append(np.mean(episode_loss) if episode_loss else 0)\n",
        "        epsilon_values.append(agent.epsilon)\n",
        "        \n",
        "        # Print progress\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-50:])\n",
        "            avg_loss = np.mean(episode_losses[-50:]) if episode_losses[-50:] else 0\n",
        "            print(f\"Episode {episode + 1:4d} | \"\n",
        "                  f\"Avg Reward: {avg_reward:7.2f} | \"\n",
        "                  f\"Avg Loss: {avg_loss:7.4f} | \"\n",
        "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
        "                  f\"Memory: {len(agent.memory)}\")\n",
        "    \n",
        "    \n",
        "    return {\n",
        "        'rewards': episode_rewards,\n",
        "        'lengths': episode_lengths,\n",
        "        'losses': episode_losses,\n",
        "        'epsilon': epsilon_values\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting MLP-based DQN Training...\n",
            "Episode   50 | Avg Reward:  -36.00 | Avg Loss:  0.1484 | Epsilon: 0.910 | Memory: 1000\n",
            "Episode  100 | Avg Reward:  -34.00 | Avg Loss:  1.8986 | Epsilon: 0.820 | Memory: 2000\n",
            "Episode  150 | Avg Reward:  -50.00 | Avg Loss:  2.2278 | Epsilon: 0.730 | Memory: 3000\n",
            "Episode  200 | Avg Reward:  -18.50 | Avg Loss:  2.4988 | Epsilon: 0.640 | Memory: 4000\n",
            "Episode  250 | Avg Reward:  -10.00 | Avg Loss:  2.3205 | Epsilon: 0.550 | Memory: 5000\n",
            "Episode  300 | Avg Reward:   -3.00 | Avg Loss:  2.1327 | Epsilon: 0.460 | Memory: 6000\n",
            "Episode  350 | Avg Reward:    1.00 | Avg Loss:  2.1351 | Epsilon: 0.370 | Memory: 7000\n",
            "Episode  400 | Avg Reward:    1.50 | Avg Loss:  2.1251 | Epsilon: 0.280 | Memory: 8000\n",
            "Episode  450 | Avg Reward:    3.00 | Avg Loss:  1.9866 | Epsilon: 0.190 | Memory: 9000\n",
            "Episode  500 | Avg Reward:    3.00 | Avg Loss:  1.9541 | Epsilon: 0.100 | Memory: 10000\n",
            "Episode  550 | Avg Reward:    4.50 | Avg Loss:  1.9375 | Epsilon: 0.100 | Memory: 11000\n",
            "Episode  600 | Avg Reward:    4.50 | Avg Loss:  1.8264 | Epsilon: 0.100 | Memory: 12000\n",
            "Episode  650 | Avg Reward:    4.50 | Avg Loss:  1.8823 | Epsilon: 0.100 | Memory: 13000\n",
            "Episode  700 | Avg Reward:    3.50 | Avg Loss:  1.8868 | Epsilon: 0.100 | Memory: 14000\n",
            "Episode  750 | Avg Reward:    4.50 | Avg Loss:  1.7996 | Epsilon: 0.100 | Memory: 15000\n",
            "Episode  800 | Avg Reward:    4.50 | Avg Loss:  1.7383 | Epsilon: 0.100 | Memory: 16000\n",
            "Episode  850 | Avg Reward:    3.50 | Avg Loss:  1.7921 | Epsilon: 0.100 | Memory: 17000\n",
            "Episode  900 | Avg Reward:    3.50 | Avg Loss:  1.6734 | Epsilon: 0.100 | Memory: 18000\n",
            "Episode  950 | Avg Reward:    4.00 | Avg Loss:  1.6759 | Epsilon: 0.100 | Memory: 19000\n",
            "Episode 1000 | Avg Reward:    3.50 | Avg Loss:  1.5619 | Epsilon: 0.100 | Memory: 20000\n",
            "Episode 1050 | Avg Reward:    4.00 | Avg Loss:  1.5018 | Epsilon: 0.100 | Memory: 21000\n",
            "Episode 1100 | Avg Reward:    4.50 | Avg Loss:  1.4616 | Epsilon: 0.100 | Memory: 22000\n",
            "Episode 1150 | Avg Reward:    5.00 | Avg Loss:  1.4350 | Epsilon: 0.100 | Memory: 23000\n",
            "Episode 1200 | Avg Reward:    4.50 | Avg Loss:  1.3946 | Epsilon: 0.100 | Memory: 24000\n",
            "Episode 1250 | Avg Reward:    4.50 | Avg Loss:  1.2907 | Epsilon: 0.100 | Memory: 25000\n",
            "Episode 1300 | Avg Reward:    2.50 | Avg Loss:  1.2767 | Epsilon: 0.100 | Memory: 26000\n",
            "Episode 1350 | Avg Reward:   -5.00 | Avg Loss:  1.1620 | Epsilon: 0.100 | Memory: 27000\n",
            "Episode 1400 | Avg Reward:  -19.00 | Avg Loss:  1.2036 | Epsilon: 0.100 | Memory: 28000\n",
            "Episode 1450 | Avg Reward:  -32.50 | Avg Loss:  1.1363 | Epsilon: 0.100 | Memory: 29000\n",
            "Episode 1500 | Avg Reward:  -35.50 | Avg Loss:  1.1990 | Epsilon: 0.100 | Memory: 30000\n",
            "Episode 1550 | Avg Reward:  -39.00 | Avg Loss:  1.2027 | Epsilon: 0.100 | Memory: 31000\n",
            "Episode 1600 | Avg Reward: -109.50 | Avg Loss:  1.2493 | Epsilon: 0.100 | Memory: 32000\n",
            "Episode 1650 | Avg Reward:  -41.00 | Avg Loss:  1.4599 | Epsilon: 0.100 | Memory: 33000\n",
            "Episode 1700 | Avg Reward:  -25.00 | Avg Loss:  1.3837 | Epsilon: 0.100 | Memory: 34000\n",
            "Episode 1750 | Avg Reward:  -21.00 | Avg Loss:  1.3653 | Epsilon: 0.100 | Memory: 35000\n",
            "Episode 1800 | Avg Reward:   -1.00 | Avg Loss:  1.5158 | Epsilon: 0.100 | Memory: 36000\n",
            "Episode 1850 | Avg Reward:    5.00 | Avg Loss:  1.3907 | Epsilon: 0.100 | Memory: 37000\n",
            "Episode 1900 | Avg Reward:    3.00 | Avg Loss:  1.3876 | Epsilon: 0.100 | Memory: 38000\n",
            "Episode 1950 | Avg Reward:    2.50 | Avg Loss:  1.4671 | Epsilon: 0.100 | Memory: 39000\n",
            "Episode 2000 | Avg Reward:    5.00 | Avg Loss:  1.4352 | Epsilon: 0.100 | Memory: 40000\n"
          ]
        }
      ],
      "source": [
        "# Initialize environment and MLP-based agent\n",
        "env = GridEnvironment_stochastic()\n",
        "\n",
        "agent_mlp = DQNAgent_MLP(\n",
        "    input_size=3,          \n",
        "    action_size=6,          \n",
        "    hidden_size=64,         \n",
        "    learning_rate=0.0001,   \n",
        "    gamma=0.95,              \n",
        "    epsilon_start=1.0,       \n",
        "    epsilon_end=0.1,         \n",
        "    epsilon_decay_steps=10000,\n",
        "    batch_size=32,    \n",
        "    target_update=500,    \n",
        "    reward_clip=True       \n",
        ")\n",
        "\n",
        "# Train the agent\n",
        "# Reference: Training aligned with environment timesteps (Mnih et al. 2015)\n",
        "training_stats_mlp = train_dqn_mlp(env, agent_mlp, n_episodes=2000, max_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Save the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('trained_models', exist_ok=True)\n",
        "\n",
        "checkpoint_path = '/CSE546/Assignment/A2/trained_models/dqn_mlp_full_checkpoint.pth'\n",
        "torch.save({\n",
        "    'policy_net_state_dict': agent_mlp.policy_net.state_dict(),\n",
        "    'target_net_state_dict': agent_mlp.target_net.state_dict(),\n",
        "    'optimizer_state_dict': agent_mlp.optimizer.state_dict(),\n",
        "    'epsilon': agent_mlp.epsilon,\n",
        "    'config': {\n",
        "        'input_size': agent_mlp.input_size,\n",
        "        'action_size': agent_mlp.action_size,\n",
        "        'gamma': agent_mlp.gamma,\n",
        "        'batch_size': agent_mlp.batch_size,\n",
        "        'target_update': agent_mlp.target_update\n",
        "    }\n",
        "}, checkpoint_path)\n",
        "print(f\"Saved full checkpoint -> {checkpoint_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
